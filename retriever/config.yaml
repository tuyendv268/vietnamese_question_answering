general:
  batch_size: 8
  device: cpu
  mode: train
  epoch: 8
  # cross = dual
  model_type: "dual"
  valid_size: 0.01
  test_size: 0.1
  max_length: 384
  n_worker: 0
  logging_per_steps: 5000
  save_per_steps: 5000000
  evaluate_per_steps: 50
  # envibert / xlm-roberta-base / vi-mrc-base
  plm: envibert
  accumulation_steps: 1

path: 
  pretrained_dir: ../pretrained
  train_data: /mnt/sda2/datas/mbf_ir/retriever/data/bin-data/test/data.data
  test_data: /mnt/sda2/datas/mbf_ir/retriever/data/bin-data/test/data.data
  val_data: /mnt/sda2/datas/mbf_ir/retriever/data/bin-data/test/data.data
  # pretrained_dir: /content/drive/MyDrive/pretrained
  # train_data: /content/vietnamese_question_answering/data/train/data.data
  # test_data: /content/vietnamese_question_answering/data/test/data.data
  # val_data: /content/vietnamese_question_answering/data/test/data.data
  warm_up: checkpoints/cross_1.bin
  ckpt: outputs/checkpoints
  log: outputs/logs
  bm25: outputs/bm25
  embedd_model: outputs/checkpoints/dual_0.bin

data_augmentation_args:
  augmentation_percent: 0.6
  mask_percent: 0.05
