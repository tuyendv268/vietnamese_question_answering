general:
  batch_size: 1
  device: cpu
  mode: train
  epoch: 2
  # cross = dual
  model_type: "cross"
  valid_size: 0.01
  test_size: 0.1
  max_length: 384
  n_worker: 0
  logging_per_steps: 1000
  save_per_steps: 5000
  evaluate_per_step: 5000
  # envibert / xlm-roberta-base / vi-mrc-base
  plm: xlm-roberta-base
  accumulation_steps: 8

path: 
  pretrained_dir: ../pretrained
  train_data: /mnt/sda2/datas/mbf_ir/reranker/data/test/data.data
  test_data: /mnt/sda2/datas/mbf_ir/reranker/data/vnexpress/data.data
  val_data: /mnt/sda2/datas/mbf_ir/reranker/data/test/data.data
  # pretrained_dir: /content/drive/MyDrive/pretrained
  # train_data: /content/vietnamese_question_answering/data/train/data.data
  # test_data: /content/vietnamese_question_answering/data/test/data.data
  # val_data: /content/vietnamese_question_answering/data/test/data.data
  warm_up: outputs/checkpoints/cross_epoch=1_step=79999.bin
  ckpt: outputs/checkpoints
  log: outputs/logs
  bm25: outputs/bm25
  embedd_model: outputs/checkpoints/dual_0.bin

data_augmentation_args:
  augmentation_percent: 0.6
  mask_percent: 0.05
